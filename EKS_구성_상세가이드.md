# EKS 구성 상세 가이드

## 🎯 발표 목표
- EKS 선택 이유와 구성 방식을 체계적으로 설명
- Multi-AZ 고가용성 아키텍처 강조
- 마이크로서비스별 배포 전략 소개
- Karpenter 도입 효과 부각

---

## 📋 발표 구성 (3분)

### 1단계: 문제 정의 및 기술 선택 (30초)
### 2단계: EKS 클러스터 아키텍처 (1분)
### 3단계: 서비스 배포 구조 (1분)
### 4단계: Karpenter 도입 효과 (30초)

---

## 🎤 상세 발표 스크립트

### 🔍 1단계: 문제 정의 및 기술 선택 (30초)

```
"이제 EKS 구성에 대해 자세히 설명드리겠습니다.

저희가 해결해야 할 핵심 과제는 세 가지였습니다.
첫째, API, Gateway, Pay 서비스를 독립적으로 관리하면서도
서로 통신할 수 있는 마이크로서비스 환경 구축,
둘째, 단일 장애점을 제거한 고가용성 시스템 설계,
셋째, 트래픽 변화에 따른 자동 확장이었습니다.

이를 해결하기 위해 AWS의 관리형 Kubernetes 서비스인 EKS를 선택했습니다."
```

### 🏗️ 2단계: EKS 클러스터 아키텍처 (1분)

```
"먼저 전체 클러스터 아키텍처를 보시겠습니다.

저희는 ap-northeast-2a와 2b, 두 개의 가용영역에 걸쳐 클러스터를 구성했습니다.
각 가용영역에는 프라이빗 서브넷과 퍼블릭 서브넷을 분리하여
보안성과 가용성을 동시에 확보했습니다.

워커 노드는 Karpenter가 관리하며, 
필요에 따라 t3.medium부터 c5.large까지 다양한 인스턴스 타입을 활용합니다.
특히 Spot 인스턴스를 우선적으로 사용하여 비용을 최적화했습니다.

네트워크는 AWS Load Balancer Controller를 통해 
Application Load Balancer와 연동되어 외부 트래픽을 분산 처리합니다."
```

### ⚙️ 3단계: 서비스 배포 구조 (1분)

```
"다음으로 각 서비스의 배포 구조를 설명드리겠습니다.

저희는 API, Gateway, Pay 세 개의 핵심 서비스를 
완전히 독립적인 Deployment로 분리했습니다.

각 서비스는 자체 ConfigMap과 Secret을 가지고 있어
환경별 설정을 독립적으로 관리할 수 있습니다.
개발, 스테이징, 프로덕션 환경의 차이점을 
코드 변경 없이 설정만으로 처리할 수 있죠.

서비스 간 통신은 Kubernetes의 내부 DNS를 활용하며,
각 서비스는 ClusterIP 타입의 Service로 노출됩니다.
외부 접근이 필요한 API와 Gateway만 LoadBalancer 타입으로 설정했습니다.

특히 고가용성을 위해 각 서비스의 Pod들이 
서로 다른 가용영역에 분산 배치되도록 설정했습니다."
```

### 🚀 4단계: Karpenter 도입 효과 (30초)

```
"마지막으로 Karpenter 도입 효과입니다.

기존 Cluster Autoscaler 대비 세 가지 주요 개선점이 있었습니다.
첫째, 노드 프로비저닝 시간이 3분에서 30초로 단축되었습니다.
둘째, 인스턴스 타입 선택이 훨씬 유연해져서 
워크로드에 최적화된 노드를 선택할 수 있게 되었고,
셋째, Spot 인스턴스 활용률이 높아져 비용이 약 60% 절감되었습니다."
```

---

## 📊 슬라이드별 상세 구성

### 슬라이드 1: "왜 EKS인가?"

**제목**: "컨테이너 오케스트레이션 솔루션 선택"

**내용**:
```
도전 과제:
• 마이크로서비스 독립 배포 및 관리
• 고가용성 시스템 구축
• 자동 확장 및 복구

EKS 선택 이유:
✓ AWS 관리형 서비스 (운영 부담 최소화)
✓ Kubernetes 표준 API (벤더 중립적)
✓ AWS 서비스와의 완벽한 통합
✓ 24/7 가용성과 자동 패치
```

**시각 자료**:
- EKS vs 셀프 관리 Kubernetes 비교표
- AWS 서비스 통합 아이콘들

### 슬라이드 2: "EKS 클러스터 아키텍처"

**제목**: "Multi-AZ 고가용성 클러스터 설계"

**내용**:
```
클러스터 구성:
• Region: ap-northeast-2 (서울)
• Availability Zones: 2a, 2b
• Kubernetes Version: 1.32

네트워크 구성:
• VPC: 10.0.0.0/16
• Private Subnets: 각 AZ별 /24
• Public Subnets: ALB 및 NAT Gateway용

보안 구성:
• Worker Node: Private Subnet Only
• EKS API Server: Public + Private Access
• IAM Roles: IRSA (IAM Roles for Service Accounts)
```

**다이어그램**:
```
┌─────────────────── VPC (10.0.0.0/16) ──────────────────┐
│                                                        │
│  ┌──── AZ-2a ────┐           ┌──── AZ-2b ────┐         │
│  │               │           │               │         │
│  │ Public Subnet │           │ Public Subnet │         │
│  │ 10.0.10.0/24  │           │ 10.0.20.0/24  │         │
│  │      │        │           │      │        │         │
│  │   [ALB]       │           │   [NAT-GW]    │         │
│  │      │        │           │      │        │         │
│  │ ──────────────┼───────────┼──────────────  │         │
│  │               │           │               │         │
│  │ Private Subnet│           │Private Subnet │         │
│  │ 10.0.30.0/24  │           │ 10.0.40.0/24  │         │
│  │               │           │               │         │
│  │ ┌─[Worker]─┐  │           │ ┌─[Worker]─┐  │         │
│  │ │ API Pod  │  │           │ │Gate Pod  │  │         │
│  │ │ Pay Pod  │  │           │ │API Pod   │  │         │
│  │ └─────────┘   │           │ └─────────┘   │         │
│  └───────────────┘           └───────────────┘         │
│                                                        │
│              ┌───────────────┐                         │
│              │  EKS Control  │                         │
│              │    Plane      │                         │
│              │  (AWS Managed)│                         │
│              └───────────────┘                         │
└────────────────────────────────────────────────────────┘
```

### 슬라이드 3: "마이크로서비스 배포 구조"

**제목**: "독립적이면서도 연결된 서비스 아키텍처"

**내용**:
```
서비스별 구성:
┌─────────────┬──────────────┬──────────────┐
│   API       │   Gateway    │     Pay      │
├─────────────┼──────────────┼──────────────┤
│ NestJS      │ NestJS       │ Node.js      │
│ TypeORM     │ Socket.IO    │ TypeScript   │
│ PostgreSQL  │ Redis Queue  │ RabbitMQ     │
│ Port: 4000  │ Port: 3000   │ Port: 3100   │
└─────────────┴──────────────┴──────────────┘

배포 전략:
• 각 서비스별 독립 Deployment
• 롤링 업데이트 (maxUnavailable: 1)
• Health Check (Liveness/Readiness Probe)
• 환경별 ConfigMap/Secret 분리
```

**다이어그램**:
```
┌──────────────── Kubernetes Cluster ────────────────┐
│                                                    │
│  ┌─── Namespace: flash-ticket ───┐                 │
│  │                               │                 │
│  │  ┌─[API Service]─┐             │                 │
│  │  │               │             │                 │
│  │  │ ┌─[Pod]─┐     │             │                 │
│  │  │ │API-1  │     │   ┌─[Gateway Service]─┐      │
│  │  │ └───────┘     │   │                   │      │
│  │  │ ┌─[Pod]─┐     │   │ ┌─[Pod]─┐         │      │
│  │  │ │API-2  │◄────┼───┤ │Gate-1 │         │      │
│  │  │ └───────┘     │   │ └───────┘         │      │
│  │  └───────────────┘   │ ┌─[Pod]─┐         │      │
│  │                      │ │Gate-2 │         │      │
│  │  ┌─[Pay Service]─┐   │ └───────┘         │      │
│  │  │               │   └───────────────────┘      │
│  │  │ ┌─[Pod]─┐     │                              │
│  │  │ │Pay-1  │◄────┼──────────────────────────────┤
│  │  │ └───────┘     │                              │
│  │  │ ┌─[Pod]─┐     │   ┌─[ConfigMap]─┐            │
│  │  │ │Pay-2  │     │   │ app-config  │            │
│  │  │ └───────┘     │   └─────────────┘            │
│  │  └───────────────┘   ┌─[Secret]────┐            │
│  │                      │ app-secret  │            │
│  │                      └─────────────┘            │
│  └───────────────────────────────────────────────┘   │
│                                                    │
│         ┌─[External Services]─┐                    │
│         │  PostgreSQL (RDS)   │                    │
│         │  Redis (ElastiCache)│                    │
│         │  RabbitMQ (MQ)      │                    │
│         └─────────────────────┘                    │
└────────────────────────────────────────────────────┘
```

### 슬라이드 4: "Karpenter 기반 노드 관리"

**제목**: "차세대 Kubernetes 오토스케일러"

**내용**:
```
Karpenter vs Cluster Autoscaler:

┌─────────────────┬──────────────┬─────────────────┐
│     기능        │ Karpenter    │ Cluster Auto.   │
├─────────────────┼──────────────┼─────────────────┤
│ 프로비저닝 시간  │ ~30초        │ 2-3분           │
│ 인스턴스 선택   │ 자동 최적화   │ 사전 정의 필요   │
│ Spot 지원       │ ✅ 완벽       │ ⚠️  제한적       │
│ 비용 최적화     │ ✅ 우수       │ ⚡ 보통          │
│ 설정 복잡도     │ ⚡ 간단        │ ⚠️  복잡         │
└─────────────────┴──────────────┴─────────────────┘

NodePool 구성:
• default: 일반 워크로드 (t3.medium, spot 우선)
• peak-time: 트래픽 급증시 (c5.large, on-demand)
```

**다이어그램**:
```
┌──────────── Karpenter NodePool Strategy ────────────┐
│                                                     │
│  ┌─[Default NodePool]─┐    ┌─[Peak NodePool]─┐      │
│  │                    │    │                 │      │
│  │ Instance Types:    │    │ Instance Types: │      │
│  │ • t3.medium       │    │ • c5.large      │      │
│  │ • t3.large        │    │ • c5.xlarge     │      │
│  │                    │    │                 │      │
│  │ Capacity:          │    │ Capacity:       │      │
│  │ • Spot: 80%       │    │ • On-Demand:60% │      │
│  │ • On-Demand: 20%  │    │ • Spot: 40%     │      │
│  │                    │    │                 │      │
│  │ Zones:             │    │ Zones:          │      │
│  │ • ap-northeast-2a  │    │ • ap-northeast-2a│     │
│  │ • ap-northeast-2b  │    │ • ap-northeast-2b│     │
│  └────────────────────┘    └─────────────────┘      │
│             │                        │              │
│             ▼                        ▼              │
│    ┌─[Normal Traffic]─┐      ┌─[Peak Traffic]─┐     │
│    │                  │      │                │     │
│    │ CPU: < 70%       │      │ CPU: > 70%     │     │
│    │ Pods: 2-8        │      │ Pods: 6-18     │     │
│    │ Cost: ~$50/month │      │ Cost: ~$200/hr │     │
│    └──────────────────┘      └────────────────┘     │
└─────────────────────────────────────────────────────┘
```

### 슬라이드 5: "배포 및 설정 관리"

**제목**: "GitOps 기반 선언적 배포"

**내용**:
```
GitOps 워크플로우:
1. 개발자가 코드 커밋
2. GitHub Actions가 이미지 빌드
3. ECR에 이미지 푸시
4. GitOps 리포지토리 업데이트
5. ArgoCD가 변경사항 감지
6. EKS 클러스터에 자동 배포

장점:
✓ 선언적 배포 (Infrastructure as Code)
✓ 배포 히스토리 추적 가능
✓ 롤백 및 복구 간편
✓ 다중 환경 관리 용이
```

---

## 🎨 다이어그램 스타일 가이드

### 색상 코드
```
- AWS 오렌지: #FF9900 (강조)
- AWS 블루: #232F3E (기본)
- 성공 그린: #7AA116 (정상 상태)
- 경고 옐로우: #FFB546 (주의)
- 오류 레드: #D13212 (문제)
```

### 아이콘 활용
```
🔧 = 설정/구성
⚡ = 성능/속도
🔒 = 보안
💰 = 비용
📊 = 모니터링
🚀 = 배포
```

---

## 🤔 예상 질문 & 답변

### Q1: "EKS 비용이 많이 나가지 않나요?"
**A**: "EKS 컨트롤 플레인 자체는 시간당 $0.10($72/월)입니다. 하지만 Karpenter로 Spot 인스턴스를 적극 활용하고, 트래픽이 없을 때는 자동으로 스케일 다운되어 전체적으로는 비용 효율적입니다. 실제로 평상시 운영비는 월 $50 내외로 유지됩니다."

### Q2: "Multi-AZ 구성의 실제 효과는?"
**A**: "실제로 한 가용영역에 장애가 발생해도 서비스가 중단되지 않았습니다. 또한 로드밸런서가 자동으로 정상 AZ로 트래픽을 라우팅해서 사용자는 장애를 느끼지 못했습니다. 가용성 99.9% 이상을 달성했습니다."

### Q3: "Kubernetes 러닝커브는 어떻게 극복했나요?"
**A**: "처음에는 어려웠지만, 팀 내에서 멘토-멘티 시스템을 운영했습니다. 제가 인프라를 담당하면서 배운 내용을 다른 팀원들과 공유하고, 실습을 통해 함께 학습했습니다. 특히 kubectl 명령어와 YAML 작성법을 중점적으로 교육했습니다."

### Q4: "서비스 간 통신 보안은 어떻게 처리했나요?"
**A**: "Kubernetes의 Network Policy를 적용해서 서비스 간 통신을 제한했습니다. 또한 모든 내부 통신은 ClusterIP를 통해 이루어지므로 외부에서 직접 접근이 불가능합니다. 민감한 데이터는 Secret으로 관리하고 있습니다."

---

## 📈 성과 지표

### 정량적 성과
```
• 노드 프로비저닝 시간: 3분 → 30초 (83% 단축)
• 인프라 비용: 60% 절감 (Spot 인스턴스 활용)
• 서비스 가용성: 99.9% 달성
• 배포 시간: 10분 → 3분 (70% 단축)
• 복구 시간: 수동 30분 → 자동 2분 (93% 단축)
```

### 정성적 성과
```
• 개발팀의 인프라 의존성 제거
• 환경별 설정 관리 자동화
• 장애 대응 프로세스 표준화
• 모니터링 및 로깅 중앙화
• GitOps 기반 투명한 배포 프로세스
```